{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fba21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/data'\n",
    "\n",
    "# places_k = pd.read_parquet(f\"{output_path}/stay_locations_k_places.parquet\")\n",
    "places_k = pd.read_parquet(f\"{output_path}/places_k.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a158c4d",
   "metadata": {},
   "source": [
    "## Calculate $q_K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1beb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qk_alignment(places_df, user_col='user_id', k_freq_col='k_freq', k_dist_col='k_dist'):\n",
    "    \"\"\"\n",
    "    Calculate qK alignment using both Jaccard similarityfor each user\n",
    "    \n",
    "    This function:\n",
    "    1. Classifies each place into categories based on K-freq and K-dist indicators\n",
    "    2. Calculates Jaccard similarity for each user\n",
    "    3. Returns user-level alignment metrics and place categorizations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    places_df : DataFrame\n",
    "        Stay locations dataframe with user_id and K-place indicators\n",
    "    user_col : str\n",
    "        Column name for user identifier\n",
    "    k_freq_col : str\n",
    "        Column name for K-freq indicator (0 or 1)\n",
    "    k_dist_col : str\n",
    "        Column name for K-dist indicator (0 or 1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (user_alignment_df, places_with_categories_df)\n",
    "        - user_alignment_df: User-level qK metrics\n",
    "        - places_with_categories_df: Original dataframe with place categories added\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”„ CALCULATING qK ALIGNMENT METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    places_analysis = places_df.copy()\n",
    "    \n",
    "    # Ensure K-place indicators are binary (0 or 1)\n",
    "    places_analysis[k_freq_col] = places_analysis[k_freq_col].fillna(0).astype(int)\n",
    "    places_analysis[k_dist_col] = places_analysis[k_dist_col].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"Total places: {len(places_analysis)}\")\n",
    "    print(f\"Users: {places_analysis[user_col].nunique()}\")\n",
    "    \n",
    "    # Step 1: Assign place categories based on K-freq and K-dist values\n",
    "    def get_place_category(row):\n",
    "        k_freq = row[k_freq_col]\n",
    "        k_dist = row[k_dist_col]\n",
    "        \n",
    "        if k_freq == 1 and k_dist == 1:\n",
    "            return 'f1d1'  # Both methods identify this place\n",
    "        elif k_freq == 1 and k_dist == 0:\n",
    "            return 'f1d0'  # Only K-freq identifies this place\n",
    "        elif k_freq == 0 and k_dist == 1:\n",
    "            return 'f0d1'  # Only K-dist identifies this place\n",
    "        elif k_freq == 0 and k_dist == 0:\n",
    "            return 'f0d0'  # Neither method identifies this place\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    places_analysis['k_type'] = places_analysis.apply(get_place_category, axis=1)\n",
    "    \n",
    "    # Step 2: Aggregate by user to count places in each category\n",
    "    user_place_counts = places_analysis.groupby(user_col)['k_type'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Ensure all categories exist in the dataframe\n",
    "    for category in ['f1d1', 'f1d0', 'f0d1', 'f0d0']:\n",
    "        if category not in user_place_counts.columns:\n",
    "            user_place_counts[category] = 0\n",
    "    \n",
    "    user_place_counts = user_place_counts.reset_index()\n",
    "    \n",
    "    print(f\"Place category distribution:\")\n",
    "    for category in ['f1d1', 'f1d0', 'f0d1', 'f0d0']:\n",
    "        total_places = user_place_counts[category].sum()\n",
    "        print(f\"  {category}: {total_places} places\")\n",
    "    \n",
    "    # Step 3: Calculate Jaccard similarity for each user\n",
    "    def calculate_jaccard(row):\n",
    "        \"\"\"Jaccard = |A âˆ© B| / |A âˆª B| = f1d1 / (f1d1 + f1d0 + f0d1)\"\"\"\n",
    "        numerator = row['f1d1']\n",
    "        denominator = row['f1d1'] + row['f1d0'] + row['f0d1']\n",
    "        return numerator / denominator if denominator > 0 else 0.0\n",
    "    \n",
    "    # Calculate alignment metrics\n",
    "    user_place_counts['jaccard'] = user_place_counts.apply(calculate_jaccard, axis=1)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    user_place_counts['total_k_places'] = user_place_counts['f1d1'] + user_place_counts['f1d0'] + user_place_counts['f0d1']\n",
    "    user_place_counts['total_places'] = user_place_counts['f1d1'] + user_place_counts['f1d0'] + user_place_counts['f0d1'] + user_place_counts['f0d0']\n",
    "    \n",
    "    # Calculate ratios\n",
    "    user_place_counts['f1d1_ratio'] = user_place_counts['f1d1'] / user_place_counts['total_places']\n",
    "    user_place_counts['f1d0_ratio'] = user_place_counts['f1d0'] / user_place_counts['total_places']\n",
    "    user_place_counts['f0d1_ratio'] = user_place_counts['f0d1'] / user_place_counts['total_places']\n",
    "    user_place_counts['f0d0_ratio'] = user_place_counts['f0d0'] / user_place_counts['total_places']\n",
    "    \n",
    "    # Calculate K-method coverage\n",
    "    user_place_counts['k_freq_coverage'] = (user_place_counts['f1d1'] + user_place_counts['f1d0']) / user_place_counts['total_places']\n",
    "    user_place_counts['k_dist_coverage'] = (user_place_counts['f1d1'] + user_place_counts['f0d1']) / user_place_counts['total_places']\n",
    "    \n",
    "    print(f\"\\n ALIGNMENT SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Users analyzed: {len(user_place_counts)}\")\n",
    "    print(f\"Jaccard similarity - Mean: {user_place_counts['jaccard'].mean():.4f}, Median: {user_place_counts['jaccard'].median():.4f}\")\n",
    "    \n",
    "\n",
    "    return user_place_counts, places_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de61731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate qK alignment using the no-work variants\n",
    "user_qk, places_with_categories = calculate_qk_alignment(\n",
    "    places_df=places_k,\n",
    "    user_col='user_id', \n",
    "    k_freq_col='k_freq',  # Using no-work version\n",
    "    k_dist_col='k_dist'   # Using no-work version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da63aa",
   "metadata": {},
   "source": [
    "### Aggregating at grid level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the grid data\n",
    "grid_df = gpd.read_parquet(\n",
    "    'grid_data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_qk = user_qk.groupby(['grd_id']).agg({\n",
    "    # general metrics\n",
    "    'user_id': 'nunique',\n",
    "\n",
    "    # qK metrics\n",
    "    'jaccard': 'mean',\n",
    "\n",
    "    'jaccard_nw': 'mean',\n",
    "    \n",
    "}).reset_index()\n",
    "\n",
    "grid_analysis = grid_qk.merge(\n",
    "    grid_df[['grd_id', 'city', 'population']],\n",
    "    left_on='grd_id',\n",
    "    right_on='grd_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aad9e5",
   "metadata": {},
   "source": [
    "## $q_K$ null model\n",
    "The null model fixed distance-visitation probability for each user, but randomly shuffle the visitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract overall distance-frequency pattern using binning approach\n",
    "print(\"ğŸ”„ EXTRACTING OVERALL DISTANCE-FREQUENCY PATTERN WITH EQUAL BINNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use your existing data - places_k contains the mobility data\n",
    "places_data = places_k\n",
    "\n",
    "# Filter for valid data\n",
    "valid_data = places_data[\n",
    "    (places_data['home_dist'].notna()) & \n",
    "    (places_data['visit_freq'] >= 2) &\n",
    "    (places_data['home_dist'] > 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"Valid records for analysis: {len(valid_data)}\")\n",
    "\n",
    "# Extract distance and frequency for all users combined\n",
    "distances = valid_data['home_dist'].values\n",
    "frequencies = valid_data['visit_freq'].values\n",
    "\n",
    "# Log-transform for power law fitting\n",
    "log_dist = np.log10(distances)\n",
    "log_freq = np.log10(frequencies)\n",
    "\n",
    "# Create equally spaced bins for log-transformed data (20 equal pieces)\n",
    "n_bins = 20\n",
    "\n",
    "# Equal spacing in log space for distances\n",
    "log_dist_min, log_dist_max = log_dist.min(), log_dist.max()\n",
    "log_distance_bins = np.linspace(log_dist_min, log_dist_max, n_bins + 1)\n",
    "\n",
    "# Equal spacing in log space for frequencies  \n",
    "log_freq_min, log_freq_max = log_freq.min(), log_freq.max()\n",
    "log_frequency_bins = np.linspace(log_freq_min, log_freq_max, n_bins + 1)\n",
    "\n",
    "print(f\"Using {n_bins} equal bins\")\n",
    "print(f\"Log distance range: {log_dist_min:.3f} to {log_dist_max:.3f}\")\n",
    "print(f\"Log frequency range: {log_freq_min:.3f} to {log_freq_max:.3f}\")\n",
    "\n",
    "# Assign each observation to distance bins (using log distances)\n",
    "valid_data['log_distance_bin'] = pd.cut(log_dist, bins=log_distance_bins, include_lowest=True)\n",
    "\n",
    "# Calculate bin statistics using equal spacing\n",
    "bin_stats = valid_data.groupby('log_distance_bin', observed=True).agg({\n",
    "    'home_dist': ['mean', 'median', 'count'],\n",
    "    'visit_freq': ['mean', 'median', 'std']\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "bin_stats.columns = ['_'.join(col).strip() for col in bin_stats.columns]\n",
    "bin_stats = bin_stats.reset_index()\n",
    "\n",
    "# Filter bins with sufficient observations (optional)\n",
    "min_obs_per_bin = 10\n",
    "bin_stats_filtered = bin_stats[bin_stats['home_dist_count'] >= min_obs_per_bin].copy()\n",
    "\n",
    "print(f\"Bins after filtering (â‰¥{min_obs_per_bin} obs): {len(bin_stats_filtered)}\")\n",
    "\n",
    "# Prepare data for regression on bin means\n",
    "bin_distances = bin_stats_filtered['home_dist_mean'].values\n",
    "bin_frequencies = bin_stats_filtered['visit_freq_mean'].values\n",
    "bin_weights = bin_stats_filtered['home_dist_count'].values  # Weight by number of observations\n",
    "\n",
    "# Log-transform bin means for power law fitting\n",
    "log_bin_dist = np.log10(bin_distances)\n",
    "log_bin_freq = np.log10(bin_frequencies)\n",
    "\n",
    "# Fit power law: freq = a * dist^b using weighted linear regression on binned data\n",
    "try:\n",
    "    from scipy import stats\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Option 1: Simple regression (unweighted)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(log_bin_dist, log_bin_freq)\n",
    "    \n",
    "    # Option 2: Weighted regression (weighted by bin size)\n",
    "    weighted_reg = LinearRegression()\n",
    "    weighted_reg.fit(log_bin_dist.reshape(-1, 1), log_bin_freq, sample_weight=bin_weights)\n",
    "    weighted_slope = weighted_reg.coef_[0]\n",
    "    weighted_intercept = weighted_reg.intercept_\n",
    "    \n",
    "    # Calculate RÂ² for weighted regression\n",
    "    y_pred_weighted = weighted_reg.predict(log_bin_dist.reshape(-1, 1))\n",
    "    weighted_r_squared = 1 - np.average((log_bin_freq - y_pred_weighted)**2, weights=bin_weights) / np.average((log_bin_freq - np.average(log_bin_freq, weights=bin_weights))**2, weights=bin_weights)\n",
    "    \n",
    "    # Store model parameters (using weighted version)\n",
    "    overall_model = {\n",
    "        'slope': weighted_slope,\n",
    "        'intercept': weighted_intercept,\n",
    "        'r_squared': weighted_r_squared,\n",
    "        'n_bins': len(bin_stats_filtered),\n",
    "        'n_observations': bin_weights.sum(),\n",
    "        'unweighted_slope': slope,\n",
    "        'unweighted_r_squared': r_value**2\n",
    "    }\n",
    "    \n",
    "    print(f\"Binned model (weighted): slope={weighted_slope:.3f}, RÂ²={weighted_r_squared:.3f}\")\n",
    "    print(f\"Binned model (unweighted): slope={slope:.3f}, RÂ²={r_value**2:.3f}\")\n",
    "    print(f\"Number of bins used: {len(bin_stats_filtered)}\")\n",
    "    print(f\"Total observations: {bin_weights.sum()}\")\n",
    "    \n",
    "    # Optional: Plot the binned data and fit\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot bin means with error bars (weighted by bin size)\n",
    "    plt.scatter(bin_distances, bin_frequencies, s=10, alpha=0.7, label='Bin means (equal spacing)')\n",
    "    \n",
    "    # Plot fitted line\n",
    "    x_pred = np.linspace(bin_distances.min(), bin_distances.max(), 100)\n",
    "    y_pred = 10**(weighted_intercept) * x_pred**weighted_slope\n",
    "    plt.plot(x_pred, y_pred, 'r-', label=f'Fit: freq = {10**weighted_intercept:.2f} Ã— dist^{weighted_slope:.3f}')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Distance (m)')\n",
    "    plt.ylabel('Visit Frequency')\n",
    "    plt.title(f'Equal-Binned Distance-Frequency Relationship (RÂ² = {weighted_r_squared:.3f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fitting binned model: {e}\")\n",
    "    # Use default parameters if fitting fails\n",
    "    overall_model = {\n",
    "        'slope': -1.0,  # Default negative slope\n",
    "        'intercept': 0.0,\n",
    "        'n_bins': 0,\n",
    "        'n_observations': len(valid_data)\n",
    "    }\n",
    "\n",
    "print(f\"Final model parameters: slope={overall_model['slope']:.3f}, intercept={overall_model['intercept']:.3f}\")\n",
    "\n",
    "# # Save bin statistics for further analysis\n",
    "# bin_stats_filtered.to_csv(f\"{output_path}/distance_frequency_equal_bins.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create distance-based probability function\n",
    "print(\"\\nğŸ² CREATING DISTANCE-BASED PROBABILITY FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def calculate_distance_probability(distances):\n",
    "    \"\"\"Calculate selection probability based on distance using empirical slope\"\"\"\n",
    "    # Avoid division by zero\n",
    "    distances = np.maximum(distances, 1.0)\n",
    "    \n",
    "    # Use empirical slope (usually negative) to weight by distance\n",
    "    weights = distances ** overall_model['slope']\n",
    "    \n",
    "    # Normalize to probabilities\n",
    "    probabilities = weights / weights.sum()\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "print(f\"Distance-based probability function created with slope: {overall_model['slope']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ee39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare data for K-rand generation\n",
    "print(\"\\nğŸ“ PREPARING DATA FOR K-RAND GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "amenity_list = [\n",
    "    'CIVIC_RELIGION', 'CULTURE', 'DINING', 'EDUCATION', 'FITNESS', 'GROCERIES', 'HEALTHCARE', 'RETAIL', 'SERVICE', 'TRANSPORT'\n",
    "    ]\n",
    "\n",
    "\n",
    "smallest_values = np.ones(len(amenity_list), dtype=int)\n",
    "\n",
    "# Get all available places for each user\n",
    "all_places = places_data[['user_id', 'stay_gid10', 'visit_freq', 'home_dist', 'home_gid9', 'work_gid9','name'] + amenity_list].copy()\n",
    "\n",
    "# Get user data with their K-dist places for reference\n",
    "user_k_dist_data = places_data[places_data['k_dist'] == 1].copy()\n",
    "\n",
    "print(f\"Total available places: {len(all_places)}\")\n",
    "print(f\"K-dist places: {len(user_k_dist_data)}\")\n",
    "print(f\"Users with K-dist places: {user_k_dist_data['user_id'].nunique()}\")\n",
    "\n",
    "# Get unique users for processing\n",
    "unique_users = user_k_dist_data['user_id'].unique()\n",
    "print(f\"Users to process: {len(unique_users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeedc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_users_for_qk_rand(user_qk, fraction=0.1, min_users_per_grid=1, max_users_per_grid=100, \n",
    "                            selected_cities=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Select a representative subset of users for qK rand calculation based on grid cells from specific cities\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_qk : DataFrame\n",
    "        User-level data with 'user_id' and 'name' columns\n",
    "    fraction : float\n",
    "        Fraction of users to select from each grid (default: 0.1 for 10%)\n",
    "    min_users_per_grid : int\n",
    "        Minimum number of users to select per grid (if available)\n",
    "    max_users_per_grid : int\n",
    "        Maximum number of users to select per grid\n",
    "    selected_cities : list, optional\n",
    "        List of city names to select users from. If None, uses all cities.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (selected_user_ids, selection_stats_df)\n",
    "        - selected_user_ids: List of selected user IDs for qK rand calculation\n",
    "        - selection_stats_df: DataFrame with selection statistics by city\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ¯ SELECTING USERS FOR qK RAND CALCULATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Filter for selected cities if specified\n",
    "    if selected_cities is not None:\n",
    "        print(f\"Filtering for cities: {selected_cities}\")\n",
    "        user_qk_filtered = user_qk[user_qk['name'].isin(selected_cities)].copy()\n",
    "        cities_found = user_qk_filtered['name'].unique()\n",
    "        cities_missing = set(selected_cities) - set(cities_found)\n",
    "        \n",
    "        if cities_missing:\n",
    "            print(f\"âš ï¸  Cities not found in data: {list(cities_missing)}\")\n",
    "        print(f\"âœ… Cities found: {list(cities_found)}\")\n",
    "    else:\n",
    "        user_qk_filtered = user_qk.copy()\n",
    "        print(\"Using all cities in dataset\")\n",
    "    \n",
    "    if len(user_qk_filtered) == 0:\n",
    "        print(\"âŒ No users found in selected cities!\")\n",
    "        return [], pd.DataFrame()\n",
    "    \n",
    "    # Group users by city\n",
    "    city_user_counts = user_qk_filtered.groupby('name')['user_id'].nunique().reset_index()\n",
    "    city_user_counts.columns = ['name', 'total_users']\n",
    "    \n",
    "    print(f\"Total cities: {len(city_user_counts)}\")\n",
    "    print(f\"Total users in selected cities: {user_qk_filtered['user_id'].nunique()}\")\n",
    "    print(f\"Target selection fraction: {fraction:.1%}\")\n",
    "    \n",
    "    selected_users = []\n",
    "    city_selection_stats = []\n",
    "    \n",
    "    # Process each city\n",
    "    for city_name, city_data in user_qk_filtered.groupby('name'):\n",
    "        city_users = city_data['user_id'].unique()\n",
    "        n_users_in_city = len(city_users)\n",
    "        \n",
    "        # Calculate target number of users to select from this city\n",
    "        target_users = max(\n",
    "            min_users_per_grid,\n",
    "            min(max_users_per_grid, int(n_users_in_city * fraction))\n",
    "        )\n",
    "        \n",
    "        # Don't select more users than available\n",
    "        target_users = min(target_users, n_users_in_city)\n",
    "        \n",
    "        # Randomly sample users from this city\n",
    "        if target_users > 0:\n",
    "            selected_city_users = np.random.choice(\n",
    "                city_users, \n",
    "                size=target_users, \n",
    "                replace=False\n",
    "            ).tolist()\n",
    "            \n",
    "            selected_users.extend(selected_city_users)\n",
    "            \n",
    "            city_selection_stats.append({\n",
    "                'name': city_name,\n",
    "                'total_users': n_users_in_city,\n",
    "                'selected_users': target_users,\n",
    "                'selection_rate': target_users / n_users_in_city\n",
    "            })\n",
    "    \n",
    "    # Create summary statistics\n",
    "    selection_stats_df = pd.DataFrame(city_selection_stats)\n",
    "    \n",
    "    total_selected = len(selected_users)\n",
    "    total_users_filtered = user_qk_filtered['user_id'].nunique()\n",
    "    overall_rate = total_selected / total_users_filtered if total_users_filtered > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“Š SELECTION SUMMARY:\")\n",
    "    print(f\"Users selected: {total_selected:,}\")\n",
    "    print(f\"Overall selection rate: {overall_rate:.1%}\")\n",
    "    if len(selection_stats_df) > 0:\n",
    "        print(f\"Average users per city: {selection_stats_df['selected_users'].mean():.1f}\")\n",
    "    print(f\"Cities processed: {len(selection_stats_df)}\")\n",
    "    \n",
    "    # Show city-level breakdown\n",
    "    if len(selection_stats_df) > 0:\n",
    "        print(f\"\\nğŸ™ï¸ CITY-LEVEL BREAKDOWN:\")\n",
    "        print(f\"{'City':<15} {'Total':<8} {'Selected':<10} {'Rate':<8}\")\n",
    "        print(\"-\" * 45)\n",
    "        for _, row in selection_stats_df.iterrows():\n",
    "            print(f\"{row['name']:<15} {row['total_users']:<8} {row['selected_users']:<10} {row['selection_rate']:<8.1%}\")\n",
    "    \n",
    "    return selected_users, selection_stats_df\n",
    "\n",
    "# Example usage:\n",
    "# Select users from specific major cities\n",
    "major_cities = ['Helsinki', 'Espoo', 'Tampere', 'Vantaa', 'Oulu', 'Turku','JyvÃ¤skylÃ¤','Kuopio', 'Lahti', 'Pori','Joensuu','Kouvola']\n",
    "\n",
    "selected_users, selection_stats = select_users_for_qk_rand(\n",
    "    user_qk,\n",
    "    fraction=0.5,  # 50% selection rate\n",
    "    min_users_per_grid=1,  # At least 5 users per city\n",
    "    max_users_per_grid=500,  # Maximum 200 users per city\n",
    "    selected_cities=major_cities,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Selected {len(selected_users)} users for qK rand calculation from {len(major_cities)} cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4-6: Generate Multiple K-rand Scenarios Per User and Calculate Average qK\n",
    "\n",
    "print(\"\\nğŸ¯ GENERATING MULTIPLE K-RAND SCENARIOS PER USER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def select_k_rand_places_for_user(user_id, user_places, required_amenities, smallest_values):\n",
    "    \"\"\"\n",
    "    Select K-rand places for a user following K-visitation framework:\n",
    "    Randomly select places until amenity requirements are met\n",
    "    \"\"\"\n",
    "    # Calculate selection probabilities based on distance\n",
    "    distances = user_places['home_dist'].values\n",
    "    probabilities = calculate_distance_probability(distances)\n",
    "    \n",
    "    # Check for invalid probabilities and handle edge cases\n",
    "    if np.any(np.isnan(probabilities)) or np.any(np.isinf(probabilities)) or probabilities.sum() == 0:\n",
    "        # Fall back to uniform probabilities if calculation fails\n",
    "        probabilities = np.ones(len(user_places)) / len(user_places)\n",
    "    \n",
    "    # Initialize tracking\n",
    "    selected_places = []\n",
    "    cumulative_amenities = np.zeros(len(required_amenities))\n",
    "    available_indices = list(range(len(user_places)))\n",
    "    \n",
    "    # Continue selecting until requirements are met or no more places available\n",
    "    max_iterations = min(len(user_places), 50)  # Prevent infinite loops\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Check if requirements are met\n",
    "        if np.all(cumulative_amenities >= smallest_values):\n",
    "            break\n",
    "            \n",
    "        # If no more places available, break\n",
    "        if len(available_indices) == 0:\n",
    "            break\n",
    "        \n",
    "        # Select next place based on distance probabilities\n",
    "        # Update probabilities for remaining places\n",
    "        remaining_probs = probabilities[available_indices]\n",
    "        \n",
    "        # Ensure probabilities are valid and sum to 1\n",
    "        if remaining_probs.sum() == 0 or np.any(np.isnan(remaining_probs)) or np.any(np.isinf(remaining_probs)):\n",
    "            # Use uniform probabilities if issues detected\n",
    "            remaining_probs = np.ones(len(available_indices)) / len(available_indices)\n",
    "        else:\n",
    "            remaining_probs = remaining_probs / remaining_probs.sum()  # Renormalize\n",
    "        \n",
    "        try:\n",
    "            selected_idx = np.random.choice(\n",
    "                available_indices,\n",
    "                size=1,\n",
    "                p=remaining_probs\n",
    "            )[0]\n",
    "        except (ValueError, TypeError) as e:\n",
    "            # If probability calculation still fails, select randomly without probabilities\n",
    "            print(f\"Warning: Probability calculation failed for user {user_id}, using uniform selection: {e}\")\n",
    "            selected_idx = np.random.choice(available_indices)\n",
    "        \n",
    "        # Add selected place\n",
    "        selected_place = user_places.iloc[selected_idx].copy()\n",
    "        selected_places.append(selected_place)\n",
    "        \n",
    "        # Update cumulative amenities\n",
    "        place_amenities = selected_place[required_amenities].astype(float).values\n",
    "        cumulative_amenities += place_amenities\n",
    "        \n",
    "        # Remove selected place from available options\n",
    "        available_indices.remove(selected_idx)\n",
    "    \n",
    "    if len(selected_places) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result_df = pd.DataFrame(selected_places)\n",
    "    result_df['k_rand'] = 1\n",
    "    result_df['user_id'] = user_id\n",
    "    \n",
    "    # Determine completion status\n",
    "    requirements_met = np.all(cumulative_amenities >= smallest_values)\n",
    "    result_df['k_rand_status'] = 'complete' if requirements_met else 'incomplete'\n",
    "    \n",
    "    return result_df\n",
    "def calculate_user_qk_rand(user_id, user_places, user_k_dist_data, amenity_list, smallest_values, n_iterations=50):\n",
    "    \"\"\"\n",
    "    Generate multiple K-rand scenarios for a single user and calculate average qK-rand\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : str\n",
    "        User identifier\n",
    "    user_places : DataFrame\n",
    "        Available places for this user\n",
    "    user_k_dist_data : DataFrame\n",
    "        User's K-dist places for reference\n",
    "    amenity_list : list\n",
    "        List of amenity column names\n",
    "    smallest_values : array\n",
    "        Minimum required values for each amenity\n",
    "    n_iterations : int\n",
    "        Number of K-rand scenarios to generate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : User-level results with average qK metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(user_places) == 0 or len(user_k_dist_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get user's K-dist places for comparison\n",
    "    k_dist_places = set(user_k_dist_data['stay_gid10'])\n",
    "    \n",
    "    qk_results = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        try:\n",
    "            # Generate K-rand places for this iteration\n",
    "            user_k_rand = select_k_rand_places_for_user(\n",
    "                user_id, \n",
    "                user_places, \n",
    "                amenity_list, \n",
    "                smallest_values\n",
    "            )\n",
    "            \n",
    "            if user_k_rand is not None:\n",
    "                # Get K-rand places for this iteration\n",
    "                k_rand_places = set(user_k_rand['stay_gid10'])\n",
    "                \n",
    "                # Calculate overlaps\n",
    "                f1d1 = len(k_rand_places & k_dist_places)  # Both methods\n",
    "                f1d0 = len(k_rand_places - k_dist_places)  # Only K-rand\n",
    "                f0d1 = len(k_dist_places - k_rand_places)  # Only K-dist\n",
    "                \n",
    "                # Calculate Jaccard similarity (qK)\n",
    "                union_size = f1d1 + f1d0 + f0d1\n",
    "                jaccard_rand = f1d1 / union_size if union_size > 0 else 0\n",
    "                \n",
    "                # Calculate Dice coefficient\n",
    "                dice_rand = (2 * f1d1) / (2 * f1d1 + f1d0 + f0d1) if (2 * f1d1 + f1d0 + f0d1) > 0 else 0\n",
    "                \n",
    "                qk_results.append({\n",
    "                    'jaccard_rand': jaccard_rand,\n",
    "                    'dice_rand': dice_rand,\n",
    "                    'f1d1': f1d1,\n",
    "                    'f1d0': f1d0,\n",
    "                    'f0d1': f0d1,\n",
    "                    'k_rand_count': len(user_k_rand),\n",
    "                    'k_rand_status': user_k_rand['k_rand_status'].iloc[0]\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Skip this iteration if it fails\n",
    "            continue\n",
    "    \n",
    "    # Calculate averages across all successful iterations\n",
    "    if len(qk_results) > 0:\n",
    "        avg_result = {\n",
    "            'user_id': user_id,\n",
    "            'n_iterations_success': len(qk_results),\n",
    "            'jaccard_rand_mean': np.mean([r['jaccard_rand'] for r in qk_results]),\n",
    "            'jaccard_rand_std': np.std([r['jaccard_rand'] for r in qk_results]),\n",
    "            'dice_rand_mean': np.mean([r['dice_rand'] for r in qk_results]),\n",
    "            'dice_rand_std': np.std([r['dice_rand'] for r in qk_results]),\n",
    "            'f1d1_mean': np.mean([r['f1d1'] for r in qk_results]),\n",
    "            'f1d0_mean': np.mean([r['f1d0'] for r in qk_results]),\n",
    "            'f0d1_mean': np.mean([r['f0d1'] for r in qk_results]),\n",
    "            'k_rand_count_mean': np.mean([r['k_rand_count'] for r in qk_results]),\n",
    "            'k_dist_count': len(k_dist_places),\n",
    "            'complete_rate': np.mean([1 if r['k_rand_status'] == 'complete' else 0 for r in qk_results])\n",
    "        }\n",
    "        \n",
    "        return avg_result\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Process each user with multiple iterations\n",
    "print(f\"Processing {len(selected_users)} users with multiple K-rand scenarios each...\")\n",
    "\n",
    "user_qk_rand_results = []\n",
    "n_iterations_per_user = 10  # Number of K-rand scenarios per user\n",
    "\n",
    "for user_id in tqdm(selected_users, desc=\"Processing users\"):\n",
    "\n",
    "    # Get available places for this user\n",
    "    user_places = all_places[all_places['user_id'] == user_id].copy()\n",
    "    # print(user_places)\n",
    "\n",
    "    # Get user's K-dist data\n",
    "    user_k_dist = user_k_dist_data[\n",
    "        (user_k_dist_data['user_id'] == user_id) &\n",
    "        (user_k_dist_data['k_dist'] == 1)\n",
    "    ]\n",
    "    \n",
    "    if len(user_places) == 0 or len(user_k_dist) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate average qK-rand for this user\n",
    "    user_result = calculate_user_qk_rand(\n",
    "        user_id, \n",
    "        user_places, \n",
    "        user_k_dist,\n",
    "        amenity_list, \n",
    "        smallest_values, \n",
    "        n_iterations=n_iterations_per_user\n",
    "    )\n",
    "    \n",
    "    # print(user_result)\n",
    "\n",
    "    if user_result is not None:\n",
    "        user_qk_rand_results.append(user_result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "user_qk_rand_df = pd.DataFrame(user_qk_rand_results)\n",
    "\n",
    "print(f\"âœ… User-level qK-rand calculated for {len(user_qk_rand_df)} users\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nğŸ“Š USER-LEVEL QK-RAND SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean Jaccard (rand): {user_qk_rand_df['jaccard_rand_mean'].mean():.4f}\")\n",
    "print(f\"Mean Dice (rand): {user_qk_rand_df['dice_rand_mean'].mean():.4f}\")\n",
    "print(f\"Average completion rate: {user_qk_rand_df['complete_rate'].mean():.3f}\")\n",
    "\n",
    "# # Show variability statistics\n",
    "# print(f\"\\nğŸ“ˆ VARIABILITY STATISTICS:\")\n",
    "# print(f\"Average Jaccard std per user: {user_qk_rand_df['jaccard_rand_std'].mean():.4f}\")\n",
    "# print(f\"Average Dice std per user: {user_qk_rand_df['dice_rand_std'].mean():.4f}\")\n",
    "\n",
    "user_qk_rand_df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
